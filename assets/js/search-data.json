{
  
    
        "post0": {
            "title": "Title",
            "content": "import numpy as np from cvxopt import matrix as cvxopt_matrix from cvxopt import solvers as cvxopt_solvers from matplotlib import pyplot as plt from sklearn.datasets import make_circles from sklearn.svm import SVC . X, y = make_circles(n_samples = 500, noise = 0.05) randomize = np.arange(len(X)) np.random.shuffle(randomize) X = X[randomize] y = y[randomize] plt.scatter(X[:, 0], X[:, 1], c = y, marker = &#39;.&#39;) plt.show() . class SVM(object): def __init__(self, C=10, kernel=&#39;linear&#39;, sigma=1, p=3, progress = False): self.C = C self.kernel = kernel self.sigma = sigma self.p = p self.progress = progress def y_transform(self,y): y_tran = np.zeros(y.size) self.class1 = np.unique(y) for i,j in enumerate(y): if j == self.class1[0]: y_tran[i]=-1 if j == self.class1[1]: y_tran[i]=1 y_tran = y_tran.reshape(-1,1) * 1. return y_tran def fit(self,X,y): m,n = X.shape self.K = np.zeros((m, m)) for i in range(m): for j in range(m): if self.kernel == &#39;linear&#39;: self.K[i,j] = self.linear_kernel(X[i], X[j]) elif self.kernel == &#39;polynomial&#39;: self.K[i,j] = self.polynomial_kernel(X[i], X[j],self.p) elif self.kernel == &#39;gaussian&#39;: self.K[i,j] = self.gaussian_kernel(X[i], X[j],self.sigma) y = self.y_transform(y) #Converting into cvxopt format P = cvxopt_matrix(np.outer(y,y)*self.K) q = cvxopt_matrix(-np.ones((m, 1))) G = cvxopt_matrix(np.vstack((np.eye(m)*-1,np.eye(m)))) h = cvxopt_matrix(np.hstack((np.zeros(m), np.ones(m) * self.C))) A = cvxopt_matrix(y.reshape(1, -1)) b = cvxopt_matrix(np.zeros(1)) #Setting solver parameters (change default to decrease tolerance) cvxopt_solvers.options[&#39;show_progress&#39;] = self.progress #cvxopt_solvers.options[&#39;abstol&#39;] = 1e-10 #cvxopt_solvers.options[&#39;reltol&#39;] = 1e-10 #cvxopt_solvers.options[&#39;feastol&#39;] = 1e-10 sol = cvxopt_solvers.qp(P, q, G, h, A, b) a = np.array(sol[&#39;x&#39;]) sv = a &gt; 1e-5 ind = np.arange(len(a)).reshape(-1,1)[sv] a = a[sv] self.sv_x = np.vstack((X[:,0].reshape(-1,1)[sv],X[:,1].reshape(-1,1)[sv])).T self.sv_y = y[sv] print(&quot;%d support vectors out of %d points&quot; % (len(a), m)) self.b = 0 for n in range(len(a)): self.b += self.sv_y[n] self.b -= np.sum(a * self.sv_y * self.K[ind[n],:].reshape(-1,1)[sv]) if len(a)!=0: self.b /= len(a) self.a = a def support_vector(self): return self.sv_x def y_predict(self,X): y_predict = np.zeros(len(X)) for q in range(len(X)): s = 0 for i, j, k in zip(self.a, self.sv_y, self.sv_x): if self.kernel == &#39;linear&#39;: s += i * j * self.linear_kernel(X[q], k) elif self.kernel == &#39;polynomial&#39;: s += i * j * self.polynomial_kernel(X[q], k, self.p) elif self.kernel == &#39;gaussian&#39;: s += i * j * self.gaussian_kernel(X[q], k, self.sigma) y_predict[q] = s return (y_predict + self.b) def predict(self,X): y = np.sign(self.y_predict(X)) for i,j in enumerate(y): if j == -1: y[i]=self.class1[0] if j == 1: y[i]=self.class1[1] return y def linear_kernel(self,x1, x2): return np.dot(x1, x2) def polynomial_kernel(self,x, y, p): return (1 + np.dot(x, y)) ** p def gaussian_kernel(self,x, y, sigma): return np.exp(-np.linalg.norm(x-y)**2 / (2 * (sigma ** 2))) . svm = SVM(kernel=&#39;linear&#39;) svm.fit(X,y) #pred = svm.predict(X) . 500 support vectors out of 500 points . fig = plt.figure(figsize = (10,10)) plt.scatter(X[:, 0], X[:, 1], c = y, marker = &#39;.&#39;) plt.scatter(svm.support_vector()[:,0], svm.support_vector()[:,1], s=10, c=&quot;r&quot;) XX1, XX2 = np.meshgrid(np.linspace(-1,1,50), np.linspace(-1,1,50)) XX = np.array([[x1, x2] for x1, x2 in zip(np.ravel(XX1), np.ravel(XX2))]) Z = svm.y_predict(XX).reshape(XX1.shape) plt.contour(XX1, XX2, Z, [0.0], colors=&#39;k&#39;, linewidths=1, origin=&#39;lower&#39;) plt.contour(XX1, XX2, Z + 1, [0.0], colors=&#39;grey&#39;, linewidths=1, origin=&#39;lower&#39;) plt.contour(XX1, XX2, Z - 1, [0.0], colors=&#39;grey&#39;, linewidths=1, origin=&#39;lower&#39;) plt.axis(&quot;tight&quot;) plt.show() . svm = SVM(kernel=&#39;polynomial&#39;,C=1,p=4) svm.fit(X,y) #pred = svm.predict(X) fig = plt.figure(figsize = (10,10)) plt.scatter(X[:, 0], X[:, 1], c = y, marker = &#39;.&#39;) plt.scatter(svm.support_vector()[:,0], svm.support_vector()[:,1], s=10, c=&quot;r&quot;) XX1, XX2 = np.meshgrid(np.linspace(-1,1,50), np.linspace(-1,1,50)) XX = np.array([[x1, x2] for x1, x2 in zip(np.ravel(XX1), np.ravel(XX2))]) Z = svm.y_predict(XX).reshape(XX1.shape) plt.contour(XX1, XX2, Z, [0.0], colors=&#39;k&#39;, linewidths=1, origin=&#39;lower&#39;) plt.contour(XX1, XX2, Z + 1, [0.0], colors=&#39;grey&#39;, linewidths=1, origin=&#39;lower&#39;) plt.contour(XX1, XX2, Z - 1, [0.0], colors=&#39;grey&#39;, linewidths=1, origin=&#39;lower&#39;) plt.title(&quot;v&quot;) plt.title(&quot;Polynomial kernel, C=1, p=6&quot;) plt.axis(&quot;tight&quot;) plt.show() . 73 support vectors out of 500 points . svm = SVM(kernel=&#39;gaussian&#39;,C=100,sigma=0.1) svm.fit(X,y) #pred = svm.predict(X) fig = plt.figure(figsize = (10,10)) plt.scatter(X[:, 0], X[:, 1], c = y, marker = &#39;.&#39;) plt.scatter(svm.support_vector()[:,0], svm.support_vector()[:,1], s=10, c=&quot;r&quot;) XX1, XX2 = np.meshgrid(np.linspace(-1,1,50), np.linspace(-1,1,50)) XX = np.array([[x1, x2] for x1, x2 in zip(np.ravel(XX1), np.ravel(XX2))]) Z = svm.y_predict(XX).reshape(XX1.shape) plt.contour(XX1, XX2, Z, [0.0], colors=&#39;k&#39;, linewidths=1, origin=&#39;lower&#39;) plt.contour(XX1, XX2, Z + 1, [0.0], colors=&#39;grey&#39;, linewidths=1, origin=&#39;lower&#39;) plt.contour(XX1, XX2, Z - 1, [0.0], colors=&#39;grey&#39;, linewidths=1, origin=&#39;lower&#39;) plt.title(&quot;v&quot;) plt.title(&quot;Polynomial kernel, C=1, p=6&quot;) plt.axis(&quot;tight&quot;) plt.show() . 97 support vectors out of 500 points . clf = SVC(C = 10, kernel = &#39;rbf&#39;) clf.fit(X, y) fig = plt.figure(figsize = (10,10)) plt.scatter(X[:, 0], X[:, 1], c = y, marker = &#39;.&#39;) XX1, XX2 = np.meshgrid(np.linspace(-1,1,50), np.linspace(-1,1,50)) XX = np.array([[x1, x2] for x1, x2 in zip(np.ravel(XX1), np.ravel(XX2))]) Z = clf.predict(XX).reshape(XX1.shape) plt.contour(XX1, XX2, Z, [0.0], colors=&#39;k&#39;, linewidths=1, origin=&#39;lower&#39;) plt.axis(&quot;tight&quot;) plt.show() . c: users divay sharma appdata local programs python python37-32 lib site-packages ipykernel_launcher.py:9: UserWarning: No contour levels were found within the data range. if __name__ == &#39;__main__&#39;: .",
            "url": "https://divay9sharma.github.io/My-Blog/2020/05/23/cvxopt-Copy2.html",
            "relUrl": "/2020/05/23/cvxopt-Copy2.html",
            "date": " ‚Ä¢ May 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Covid-19 prediction and Analysis",
            "content": "In December 2019, China found the first patient infected with Covid-19(Coronavirus Disease 19). The virus is a member of novel coronaviruses. The patient has reportedly caught the virus from the seafood market in Wuhan which is the epicenter of Covid-19. Since then Covid-19 has spread all over the world. On 11 March 2020, the World Health Organization declared Covid-19 as Pandemic. Steps such as lockdown and quarantine have become imperative. But how exactly are these measures shaping the curve and what impact it will do on the future trend can be studied using certain curves. Here we try to predict the future trend and impact of lockdown measures by fitting the current cumulative cases with some particular growth curves. We will introduce these curves later. First, let&#39;s see how curve fitting is done. . What is Curve Fitting? . Curve fitting is the process of constructing a curve or mathematical function, that has the best fit to a series of data points, possibly subject to constraints. The most popular method for curve fitting is to use least-squares minimization, however other methods exist as well. Our problem is unconstrained, and according to scipy documentation, &quot;Default method to use for optimization is ‚Äòlm‚Äô for unconstrained problems and ‚Äòtrf‚Äô if bounds are provided.&quot; lm or Levenberg‚ÄìMarquardt algorithm is used to solve non-linear least squares problems. . This model is based on the research paper &quot;Prediction and analysis of Coronavirus Disease 2019&quot; by LinJia, Keen Li, Yu Jiang, Xin Guo, Ting Zhao They tested their model on the trend of SARS disease. Find it here. Since the SARS and Covid-19 are both coronaviruses, these same models can be implemented here. Lets now introduce the 3 types of the curve we will be fitting. . Growth Curves . Logistic/Sigmoid fuction | . A logistic function or logistic curve is a common S-shaped curve (sigmoid curve). It is commonly to explore the risk factors of a certain disease, and predict the probability of occurrence of a certain disease according to the risk factors. begin{align} f(x) = frac{a}{1+e^{b-c(t-t_0)}} end{align} . Bertalanffy function | . Bertalanffy model is often used as a growth model. It is used to describe the growth characteristics of fish. Other species can also be used to describe the growth of animals, such as pigs, horses, cattle, sheep, etc. and other infectious diseases. begin{align} f(x) = a(1-e^{-b(t-t_0)})^c end{align} . Gompertz function | . The model was originally proposed by Gomperts (Gompertz,1825) as an animal population growth model to describe the extinction law of the population. The development of infectious diseases is similar to the growth of individuals and populations. begin{align} f(x) = ae^{-be^{-c(t-t_0)}} end{align} . In above models, $f(x)$ are the confirmed cases, $a$ is predicted maximum confirmed cases $b$ and $c$ are hyperparameter, $t$ is number of days since first case is reported and $t_0$ is day on which first case occurred. . Model Evaluation | . The regression coefficient $(R^2)$ is used to evaluate the fitting ability of various methods and can be obtained by the following equation. begin{align} R^2 = 1- frac{ sum(y_i - hat{y_i})^2}{ sum(y_i - bar{y})^2} end{align} $ùë¶_ùëñ$ is the actual cumulative confirmed COVID-19 cases; $ hat{y_i}$ is the predicted cumulative confirmed COVID-19 cases; $ bar{y}$ is the average of the actual cumulative confirmed COVID-19 cases. The closer the fitting coefficient is to 1, the more accurate the prediction. . Lets now import essesntial librabies. . import pandas as pd import numpy as np import matplotlib.pyplot as plt from scipy.optimize import curve_fit import warnings warnings.filterwarnings(&quot;ignore&quot;) . def f_sigmoid(x, a, b, c): # c = sigmoid midpoint # b = curve steepness (logistic growth) # a = max value return (c / (1 + np.exp(b-a*(x)))) def Bertalanffy(x, a, b, c): # a = max value # b,c = function parameter return a*np.sign(1 - np.exp(-b*(x-x_0)))*np.abs(1 - np.exp(-b*(x)))**c def Gompertz(x, a, b, c): # a = max value # b,c = function parameter return a*np.exp(-b*np.exp(-c*(x))) def r2(y,y_hat): return 1-(np.sum((y-y_hat)**2)/np.sum((y - np.mean(y))**2)) . We will be using John Hopkins dataset. Find it here. . First lets see how the data looks like. . data = pd.read_csv(&#39;time_series_covid19_confirmed_global.csv&#39;) data.head() . Province/State Country/Region Lat Long 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 ... 5/8/20 5/9/20 5/10/20 5/11/20 5/12/20 5/13/20 5/14/20 5/15/20 5/16/20 5/17/20 . 0 NaN | Afghanistan | 33.0000 | 65.0000 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3778 | 4033 | 4402 | 4687 | 4963 | 5226 | 5639 | 6053 | 6402 | 6664 | . 1 NaN | Albania | 41.1533 | 20.1683 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 850 | 856 | 868 | 872 | 876 | 880 | 898 | 916 | 933 | 946 | . 2 NaN | Algeria | 28.0339 | 1.6596 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 5369 | 5558 | 5723 | 5891 | 6067 | 6253 | 6442 | 6629 | 6821 | 7019 | . 3 NaN | Andorra | 42.5063 | 1.5218 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 752 | 754 | 755 | 755 | 758 | 760 | 761 | 761 | 761 | 761 | . 4 NaN | Angola | -11.2027 | 17.8739 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 43 | 43 | 45 | 45 | 45 | 45 | 48 | 48 | 48 | 48 | . 5 rows √ó 121 columns . Dates start from 22nd January 2020. &#39;Country/Region&#39; contains the country name and if province wise data is given then &#39;Province/State&#39; contains those provinces. Notice these are cumulative cases. Dates end on 17th May 2020. So, the total days will be 117. . df=data[data[&#39;Country/Region&#39;]==&#39;India&#39;] cases = np.array(df.sum(axis = 0))[4:] cases = cases.T.reshape(cases.size) date = np.linspace(1,cases.size,cases.size) print(cases) print(&quot;Number of days&quot;,cases.size) plt.scatter(date,cases,label=&quot;India&quot;) plt.xlabel(&quot;Dates from 22/1/2020&quot;) plt.ylabel(&quot;Total Cases&quot;) plt.legend() . [0 0 0 0 0 0 0 0 1 1 1 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 28 30 31 34 39 43 56 62 73 82 102 113 119 142 156 194 244 330 396 499 536 657 727 887 987 1024 1251 1397 1998 2543 2567 3082 3588 4778 5311 5916 6725 7598 8446 9205 10453 11487 12322 13430 14352 15722 17615 18539 20080 21370 23077 24530 26283 27890 29451 31324 33062 34863 37257 39699 42505 46437 49400 52987 56351 59695 62808 67161 70768 74292 78055 81997 85784 90648 95698] Number of days 117 . &lt;matplotlib.legend.Legend at 0x134bd7b0&gt; . The curve shows an upward growth. The curve obviously cannot just keep growing as some peak will be reached once either the infection is controlled or there are no new people to infect. It seems obvious to fit the data on some modified logistic curve. . We will be using the &#39;curve_fit&#39; method of Scipy. We would plug our growth functions in the curve_fit method, which would return the optimum parameters for the given function. . In extra_date, I provide the dates up to which I would like my model to predict values. We would use the optimum parameters obtained to predict future values. . #Predict upto 200 days extra_date = np.linspace(1,200,200) #Logistic popt_sig, pcov_sig = curve_fit(f_sigmoid, date, cases) ans1 = f_sigmoid(extra_date,popt_sig[0],popt_sig[1],popt_sig[2]) #print(popt_sig) #Bertalanffy popt_sig, pcov_sig = curve_fit(Bertalanffy, date, cases) ans2 = Bertalanffy(extra_date,popt_sig[0],popt_sig[1],popt_sig[2]) #print(popt_sig) #Gompertz popt_sig, pcov_sig = curve_fit(Gompertz, date, cases) ans3 = Gompertz(extra_date,popt_sig[0],popt_sig[1],popt_sig[2]) #print(popt_sig) plt.plot(date, cases,&#39;.&#39;,color =&#39;yellow&#39;, label =&quot;data&quot;) plt.plot(extra_date, ans1, &#39;--&#39;, color =&#39;blue&#39;, label =&quot;sigmoid&quot;) plt.plot(extra_date, ans2, &#39;--&#39;, color =&#39;green&#39;, label =&quot;Bertalanffy&quot;) plt.plot(extra_date, ans3, &#39;--&#39;, color =&#39;red&#39;, label =&quot;Gompertz&quot;) plt.xlabel(&quot;Dates from 22/1/2020&quot;) plt.ylabel(&quot;Total Cases India&quot;) plt.legend() plt.show() print(&quot;R2 score&quot;,r2(cases,ans1[:cases.size])) . R2 score 0.9988028427259011 . Ok, so the graph shows two different types of curves. Sigmoid shows a very abrupt stop contrary to the other two curves. The two curves show a rapid increase in the cases. To see why this might be happening, look at the previous graph of total cases. the graph shows a rapid increase in new cases, as the data points are more spread out in later days. In the cycle of infection spread, this phase is known as the exponential growth phase. The R2 score gives us a nice value close to 1. . This phase is characterized by an exponential increase. The growth only stops if the infection is either controlled by a vaccine or quarantine measure or if there is no population left to infect. From the above graph, it can then be deduced that both Bertalanffy and Gompertz curves were able to utilize this fact to show the future trend of the infection. We will test this claim against different countries and also how close these predictions are to the actual values. . Let&#39;s see how this compares to other countries. . df=np.array(data[data[&#39;Country/Region&#39;]==&#39;India&#39;]) cases = df[:,4:] cases_ind = cases.T.reshape(cases.size) df=np.array(data[data[&#39;Country/Region&#39;]==&#39;Italy&#39;]) cases = df[:,4:] cases_it = cases.T.reshape(cases.size) df=np.array(data[data[&#39;Country/Region&#39;]==&#39;Iran&#39;]) cases = df[:,4:] cases_iran = cases.T.reshape(cases.size) df=np.array(data[data[&#39;Country/Region&#39;]==&#39;Switzerland&#39;]) cases = df[:,4:] cases_swiss = cases.T.reshape(cases.size) plt.plot(date,cases_iran,label=&quot;Iran&quot;) plt.plot(date,cases_swiss,label=&quot;Switzerland&quot;) plt.plot(date,cases_ind,label=&quot;India&quot;) plt.plot(date,cases_it,label=&quot;Italy&quot;) plt.xlabel(&quot;Dates from 22/1/2020&quot;) plt.ylabel(&quot;Confirmed cases&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0xd24d70&gt; . I have chosen these countries because they all are at different phases of infection. Switzerland has long achieved its peak. Iran is seeing another surge of increase in cases. Ilaty is about to achieve its peak. Lets now see how these the prediction graph looks like. For this, I will only use the Gompertz function. . extra_date = np.linspace(1,150,150) popt_sig, pcov_sig = curve_fit(Gompertz, date, cases_ind) ans1 = Gompertz(extra_date,popt_sig[0],popt_sig[1],popt_sig[2]) popt_sig, pcov_sig = curve_fit(Gompertz, date, cases_it) ans2 = Gompertz(extra_date,popt_sig[0],popt_sig[1],popt_sig[2]) popt_sig, pcov_sig = curve_fit(Gompertz, date, cases_iran) ans3 = Gompertz(extra_date,popt_sig[0],popt_sig[1],popt_sig[2]) popt_sig, pcov_sig = curve_fit(Gompertz, date, cases_swiss) ans4 = Gompertz(extra_date,popt_sig[0],popt_sig[1],popt_sig[2]) plt.plot(date, cases_ind,&#39;.&#39;,color =&#39;yellow&#39;) plt.plot(extra_date, ans1, color =&#39;green&#39;, label =&quot;India&quot;) plt.plot(date, cases_it,&#39;.&#39;,color =&#39;yellow&#39;) plt.plot(extra_date, ans2, color =&#39;red&#39;, label =&quot;Italy&quot;) plt.plot(date, cases_swiss,&#39;.&#39;,color =&#39;yellow&#39;) plt.plot(extra_date, ans4, color =&#39;orange&#39;, label =&quot;Switzerland&quot;) plt.plot(date, cases_iran,&#39;.&#39;,color =&#39;yellow&#39;) plt.plot(extra_date, ans3,color =&#39;blue&#39;, label =&quot;Iran&quot;) plt.xlabel(&quot;Dates from 22/1/2020&quot;) plt.ylabel(&quot;Confirmed cases&quot;) plt.legend() plt.show() . Except for India, the other countries will reach their peak in a couple of weeks. We can observe that India has a longer growth phase compared to other countries. Thus a longer growth phase in time, in turn, will correlate to a higher peak value. Also, due to lockdown measures, India&#39;s witnessed the delayed growth phase. . Lets now see how close these predictions are to real value. For this I create an iteration starting from the 50th date, I find the prediction for next +n date. What it means is that for each $i^{th}$ date i will predict for $i+n^{th}$ date. After this, we can see the difference between the actual and predicted values. . sigmoid = pd.DataFrame(columns=[&#39;date&#39;,&#39;real&#39;,&#39;pred&#39;]) n=2 for i in range(50,cases.size-n): extra_date = np.linspace(1,500,500) try: popt_sig, pcov_sig = curve_fit(f_sigmoid, date[:i], cases[:i]) ans1 = (popt_sig[2] / (1 + np.exp(popt_sig[1]-(extra_date*popt_sig[0])))) sigmoid = sigmoid.append(pd.DataFrame({&#39;date&#39;:[i],&#39;real&#39;:[cases[i+n]],&#39;pred&#39;:[ans1[i+n]]})) except RuntimeError: sigmoid = sigmoid.append(pd.DataFrame({&#39;date&#39;:[i],&#39;real&#39;:[cases[i+n]],&#39;pred&#39;:[cases[i+n]]})) continue plt.xlabel(&quot;Dates from 10/3/2020&quot;) plt.title(&quot;n=2,sigmoid fuction&quot;) plt.plot(np.array(sigmoid)[:,0], np.array(sigmoid)[:,1],&#39;.&#39;,color =&#39;red&#39;, label =&quot;Actual data&quot;) plt.plot(np.array(sigmoid)[:,0], np.array(sigmoid)[:,2],&#39;.&#39;,color =&#39;green&#39;, label =&quot;Predicted Data&quot;) plt.legend() plt.show() . Here, the difference between the actual and predicted value is very small. Prediction of close dates can be done with high accuracy. The measure of difference can be computing the percentage error by taking the difference of actual and prediction values and dividing it by actual value. . Notice that I have used the try, except statement here. This is because the curve_fit method, in some cases fails to find the optimum parameters for the given iterations. Thus the function throws a RuntimeError which is taken by the except block. To keep the consistency of the graph, when the method fails to optimize the data, I substitute the actual values in place of the predicted values. This will give us a way to see such points in the graph. . np.mean((np.array(sigmoid)[:,1] - np.array(sigmoid)[:,2])/np.array(sigmoid)[:,1]) . 0.07855719960874576 . 7.8% error. . bertalanffy = pd.DataFrame(columns=[&#39;date&#39;,&#39;real&#39;,&#39;pred&#39;]) n=8 for i in range(50,cases.size-n): extra_date = np.linspace(1,500,500) try: popt_sig, pcov_sig = curve_fit(Bertalanffy, date[:i], cases[:i]) ans2 = popt_sig[0]*(1 - np.exp(-popt_sig[1]*extra_date))**popt_sig[2] bertalanffy = bertalanffy.append(pd.DataFrame({&#39;date&#39;:[i],&#39;real&#39;:[cases[i+n]],&#39;pred&#39;:[ans2[i+n]]})) except RuntimeError: bertalanffy = bertalanffy.append(pd.DataFrame({&#39;date&#39;:[i],&#39;real&#39;:[cases[i+n]],&#39;pred&#39;:[cases[i+n]]})) continue plt.title(&quot;n=8,bertalanffy fuction&quot;) plt.plot(np.array(bertalanffy)[:,0], np.array(bertalanffy)[:,1],&#39;.&#39;,color =&#39;red&#39;, label =&quot;Actual Data&quot;) plt.plot(np.array(bertalanffy)[:,0], np.array(bertalanffy)[:,2],&#39;.&#39;,color =&#39;green&#39;, label =&quot;Predicted Data&quot;) plt.legend() plt.show() . np.mean((np.array(bertalanffy)[:,1] - np.array(bertalanffy)[:,2])/np.array(bertalanffy)[:,1]) . 0.14557860820569543 . gompertz = pd.DataFrame(columns=[&#39;date&#39;,&#39;real&#39;,&#39;pred&#39;]) n=15 for i in range(50,cases.size-n): extra_date = np.linspace(1,300,300) try: popt_sig, pcov_sig = curve_fit(Gompertz, date[:i], cases[:i]) ans3 = popt_sig[0]*np.exp(-popt_sig[1]*np.exp(-popt_sig[2]*extra_date)) gompertz = gompertz.append(pd.DataFrame({&#39;date&#39;:[i],&#39;real&#39;:[cases[i+n]],&#39;pred&#39;:[ans3[i+n]]})) except RuntimeError: gompertz = gompertz.append(pd.DataFrame({&#39;date&#39;:[i],&#39;real&#39;:[cases[i+n]],&#39;pred&#39;:[cases[i+n]]})) continue plt.title(&quot;n=15, gompertz fuction&quot;) plt.plot(np.array(gompertz)[:,0], np.array(gompertz)[:,1],&#39;.&#39;,color =&#39;red&#39;, label =&quot;Actual Data&quot;) plt.plot(np.array(gompertz)[:,0], np.array(gompertz)[:,2],&#39;.&#39;,color =&#39;green&#39;, label =&quot;Predicted Data&quot;) plt.legend() plt.show() . np.mean((np.array(gompertz)[:,1] - np.array(gompertz)[:,2])/np.array(gompertz)[:,1]) . 0.2035440918563918 . This gives us a 20% error. The error rate will be higher as for a lot of points the model failed to optimize. For days more than 10~20 models can give an approx prediction. However, in almost every case, the actual values are less than the predicted values. Thus our model can certainly give a conservative estimate of the epidemic for days more than 15. . Conclusion . From the prediction graphs we also conclude that our model has been able to fit the virus spread at all stages. Different functions reacted differently to the data. We saw that sigmoid was not able to provide a good generalization as it was capping very soon. In contrast, Bertalanffy and Gompertz gave a better result. In between these two also, Gompertz gave a more conservative result. Prediction of the peak with these functions can only give a rough estimate as predicting for more than 20 days gave an approx 20% error. . In Conclusion, given the large number of factors that could influence the future cases in reality, thus predictions for more than 10-15 days become very difficult. Almost always, factors such as government policies, climate, lockdown measures, virus mutation, availability of the vaccine, etc, can greatly influence the number of cases. If these factors remain constant and the same as they were for the past, then our model can give a good prediction for more than 20 days. Creating an online model that updates the prediction every day will also give a better estimate. .",
            "url": "https://divay9sharma.github.io/My-Blog/2020/05/18/Covid.html",
            "relUrl": "/2020/05/18/Covid.html",
            "date": " ‚Ä¢ May 18, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://divay9sharma.github.io/My-Blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it‚Äôs in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://divay9sharma.github.io/My-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://divay9sharma.github.io/My-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}